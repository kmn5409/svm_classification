{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŽ¬ IMDB Movie Review Sentiment Dataset\n",
        "\n",
        "### ðŸ§¾ Overview\n",
        "\n",
        "The **IMDB Movie Review Dataset** is a widely used benchmark in Natural Language Processing (NLP) for **binary sentiment classification** â€” determining whether a movie review expresses a **positive** or **negative** opinion.\n",
        "\n",
        "Each sample in the dataset is a single movie review written by a user on the [Internet Movie Database (IMDB)](https://www.imdb.com/), labeled as:\n",
        "\n",
        "- **Positive (1):** The review reflects enjoyment or approval of the movie.\n",
        "- **Negative (0):** The review expresses dislike or criticism of the movie.\n",
        "\n",
        "The dataset contains:\n",
        "- **25,000** labeled training reviews  \n",
        "- **25,000** labeled test reviews  \n",
        "All text data is preprocessed, and each review is already tokenized into words or indices (depending on the version you load).\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“¦ Dataset Source\n",
        "\n",
        "This dataset is included in several libraries:\n",
        "\n",
        "- **Keras / TensorFlow:**  \n",
        "  ```python\n",
        "  from tensorflow.keras.datasets import imdb\n",
        "  (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)"
      ],
      "metadata": {
        "id": "hpmKGS3RWsii"
      },
      "id": "hpmKGS3RWsii"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# If needed:\n",
        "# %pip install datasets --quiet\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_imdb_via_hf(subset_train=20000, subset_test=5000, seed=7):\n",
        "    \"\"\"Load IMDB via Hugging Face datasets, returning raw text and labels.\n",
        "    Subsample to keep runtime manageable.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from datasets import load_dataset\n",
        "        ds = load_dataset(\"imdb\")\n",
        "        # Convert to pandas for easier subsampling\n",
        "        train_df = pd.DataFrame({'text': ds['train']['text'], 'label': ds['train']['label']})\n",
        "        test_df  = pd.DataFrame({'text': ds['test']['text'],  'label': ds['test']['label']})\n",
        "        # Subsample\n",
        "        train_df = train_df.sample(n=min(subset_train, len(train_df)), random_state=seed).reset_index(drop=True)\n",
        "        test_df  = test_df.sample(n=min(subset_test,  len(test_df)),  random_state=seed).reset_index(drop=True)\n",
        "        return train_df['text'].tolist(), train_df['label'].to_numpy(), test_df['text'].tolist(), test_df['label'].to_numpy()\n",
        "    except Exception as e:\n",
        "        print(\"HF datasets not available or failed:\", e)\n",
        "        return None\n",
        "\n",
        "def load_imdb_via_keras(subset_train=20000, subset_test=5000, seed=7):\n",
        "    \"\"\"Fallback loader using Keras integer-encoded IMDB; returns decoded text strings if possible.\"\"\"\n",
        "    try:\n",
        "        # %pip install tensorflow --quiet\n",
        "        from tensorflow.keras.datasets import imdb\n",
        "        from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "        from tensorflow.keras.datasets.imdb import get_word_index\n",
        "\n",
        "        num_words = 50000\n",
        "        (X_tr_i, y_tr), (X_te_i, y_te) = imdb.load_data(num_words=num_words)\n",
        "        # Pad and decode into crude space-joined tokens\n",
        "        word_index = get_word_index()\n",
        "        index_word = {v+3: k for k,v in word_index.items()}\n",
        "        index_word[0] = \"<pad>\"; index_word[1] = \"<start>\"; index_word[2] = \"<unk>\"; index_word[3] = \"<unused>\"\n",
        "        def decode(seq):\n",
        "            return \" \".join(index_word.get(i, \"<unk>\") for i in seq)\n",
        "\n",
        "        # Subsample\n",
        "        rng = np.random.default_rng(seed)\n",
        "        tr_idx = rng.choice(len(X_tr_i), size=min(subset_train, len(X_tr_i)), replace=False)\n",
        "        te_idx = rng.choice(len(X_te_i), size=min(subset_test,  len(X_te_i)), replace=False)\n",
        "        X_tr_txt = [decode(X_tr_i[i]) for i in tr_idx]\n",
        "        X_te_txt = [decode(X_te_i[i]) for i in te_idx]\n",
        "        y_tr_s = y_tr[tr_idx]; y_te_s = y_te[te_idx]\n",
        "        return X_tr_txt, y_tr_s, X_te_txt, y_te_s\n",
        "    except Exception as e:\n",
        "        print(\"Keras IMDB not available or failed:\", e)\n",
        "        return None\n",
        "\n",
        "# Try HF first, then Keras fallback\n",
        "data = load_imdb_via_hf(subset_train=20000, subset_test=5000, seed=7)\n",
        "if data is None:\n",
        "    data = load_imdb_via_keras(subset_train=20000, subset_test=5000, seed=7)\n",
        "\n",
        "if data is None:\n",
        "    raise RuntimeError(\"Could not load IMDB via Hugging Face or Keras. Install one of them and retry.\")\n",
        "\n",
        "X_train_text, y_train, X_test_text, y_test = data\n",
        "len(X_train_text), len(X_test_text), np.bincount(y_train), np.bincount(y_test)\n"
      ],
      "metadata": {
        "id": "WlJ1hpLeRuZh"
      },
      "id": "WlJ1hpLeRuZh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_text[0][:100])"
      ],
      "metadata": {
        "id": "UPdexDMPWS69"
      },
      "id": "UPdexDMPWS69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(y_train))"
      ],
      "metadata": {
        "id": "GngfwaDoWLTx"
      },
      "id": "GngfwaDoWLTx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the unique values and their counts\n",
        "unique_values, counts = np.unique(y_test, return_counts=True)\n",
        "\n",
        "# The number of unique values is simply the length of the unique_values array\n",
        "num_unique_values = len(y_test)\n",
        "\n",
        "print(f\"Unique values: {y_test}\")\n",
        "print(f\"Counts of unique values: {counts}\")\n",
        "print(f\"Number of unique values: {num_unique_values}\")"
      ],
      "metadata": {
        "id": "8ntem9MOf8dx"
      },
      "id": "8ntem9MOf8dx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    stop_words='english',\n",
        "    max_features=50000,\n",
        "    ngram_range=(1,2),\n",
        "    min_df=2\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "H5XNb0bBR6Y-"
      },
      "id": "H5XNb0bBR6Y-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "69876f86",
      "metadata": {
        "id": "69876f86"
      },
      "source": [
        "\n",
        "### Fromâ€‘Scratch Linear SVM on TruncatedSVD Features\n",
        "\n",
        "Our NumPyâ€‘only `LinearSVM` expects dense features. Weâ€™ll reuse the SVDâ€‘reduced dense representation (200â€‘D) and train the fromâ€‘scratch model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ef4e536",
      "metadata": {
        "id": "6ef4e536"
      },
      "source": [
        "\n",
        "### ðŸ“š Math for this section â€” TFâ€‘IDF â†’ SVD (LSA) â†’ Linear SVM\n",
        "\n",
        "**TFâ€‘IDF weighting** for term \\(t\\) in document \\(d\\):\n",
        "$\n",
        "\\mathrm{tf\\!-\\!idf}(t,d) = \\mathrm{tf}(t,d)\\cdot \\underbrace{\\log\\frac{N}{\\mathrm{df}(t)+1}}_{\\mathrm{idf}(t)},\n",
        "$\n",
        "where $(\\mathrm{tf}(t,d)$) is term frequency, $(\\mathrm{df}(t)$) is document frequency, and \\(N\\) is #docs.\n",
        "\n",
        "**Truncated SVD (LSA)** on sparse documentâ€“term matrix $(X) (size (N\\times V$)):\n",
        "$\n",
        "X \\approx U_k \\Sigma_k V_k^\\top,\\quad \\text{project } \\mathbf{z}=\\mathbf{x}V_k \\in \\mathbb{R}^k.\n",
        "$\n",
        "\n",
        "**Standardization** per feature $(j): (\\tilde{z}_j = (z_j-\\mu_j)/\\sigma_j)$.\n",
        "\n",
        "**Linear SVM hinge objective (binary)** with $(y_i'\\in\\{-1,+1\\}$):\n",
        "\n",
        "$$\n",
        "\\min_{\\mathbf{w},b}\\; \\tfrac{1}{2}\\lVert \\mathbf{w}\\rVert^2 + C\\sum_i \\max(0,\\,1 - y_i'(\\mathbf{w}^\\top\\mathbf{x}_i+b)).\n",
        "$$\n",
        "\n",
        "**Subgradient updates** (per sample) with margin $(m_i=y_i'(\\mathbf{w}^\\top\\mathbf{x}_i+b)$):\n",
        "$\n",
        "\\begin{aligned}\n",
        "m_i \\ge 1 &:&& \\mathbf{w}\\leftarrow \\mathbf{w}-\\eta\\,\\mathbf{w} \\\\\n",
        "m_i < 1   &:&& \\mathbf{w}\\leftarrow \\mathbf{w}-\\eta\\,(\\mathbf{w}-C\\,y_i'\\mathbf{x}_i),\\;\\; b\\leftarrow b+\\eta\\,C\\,y_i'.\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "**Linear decision function:**\n",
        "$s =  \\mathbf{w}^\\top\\mathbf{x}_i+b$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e3a49b2",
      "metadata": {
        "id": "6e3a49b2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ---------------- From-scratch Linear SVM ----------------\n",
        "class LinearSVMScratch:\n",
        "    def __init__(self, C=1.0, lr=0.1, n_epochs=10, lr_decay=0.0, shuffle=True, random_state=7, early_stopping=False, es_patience=3):\n",
        "        self.C = float(C)\n",
        "        self.lr = float(lr)\n",
        "        self.n_epochs = int(n_epochs)\n",
        "        self.lr_decay = float(lr_decay)\n",
        "        self.shuffle = bool(shuffle)\n",
        "        self.random_state = int(random_state)\n",
        "        self.early_stopping = bool(early_stopping)\n",
        "        self.es_patience = int(es_patience)\n",
        "        self.w_ = None\n",
        "        self.b_ = 0.0\n",
        "        self.loss_curve_ = []\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_pm1(y):\n",
        "        y = np.asarray(y).astype(int)\n",
        "        if set(np.unique(y)) == {0,1}:\n",
        "            return np.where(y==1, 1, -1)\n",
        "        elif set(np.unique(y)) == {-1,1}:\n",
        "            return y\n",
        "        else:\n",
        "            raise ValueError(\"Labels must be in {0,1} or {-1,+1}.\")\n",
        "\n",
        "    def _hinge_loss(self, X, ypm1):\n",
        "        # ------------------------------------------------\n",
        "        # TODO Refer to the Linear SVM hinge objective defined in the cell above (Exercise 1)\n",
        "        # ------------------------------------------------\n",
        "        margins = 1.0 - ypm1 * ( @ self.w_ + )\n",
        "        return 0.5*np.dot(self.w_, ) + * np.maximum(0.0, margins).sum()\n",
        "        # ----------------------------\n",
        "        # Implementation Ends Here\n",
        "        # ----------------------------\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        rng = np.random.default_rng(self.random_state)\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        ypm1 = self._to_pm1(y)\n",
        "        n, d = X.shape\n",
        "        self.w_ = np.zeros(d, dtype=float)\n",
        "        self.b_ = 0.0\n",
        "        idx = np.arange(n)\n",
        "        best = np.inf\n",
        "        bad = 0\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            if self.shuffle:\n",
        "                rng.shuffle(idx)\n",
        "            eta = self.lr / (1.0 + self.lr_decay * epoch)\n",
        "\n",
        "            for i in idx:\n",
        "                xi = X[i]; yi = ypm1[i]\n",
        "                # ------------------------------------------------\n",
        "                # TODO Refer to the Subgradient updates (per sample) with margin equation in the cell above (Exercise 2)\n",
        "                # ------------------------------------------------\n",
        "                margin = yi * (np.dot(self., ) + self.b_)\n",
        "                # ----------------------------\n",
        "                # Implementation Ends Here\n",
        "                # ----------------------------\n",
        "                if margin >= 1.0:\n",
        "                    self.w_ -= eta * self.w_\n",
        "                else:\n",
        "                    self.w_ -= eta * (self.w_ - self.C * yi * xi)\n",
        "                    self.b_ += eta * self.C * yi\n",
        "\n",
        "            loss = self._hinge_loss(X, ypm1)\n",
        "            self.loss_curve_.append(float(loss))\n",
        "            print(f\"Epoch {epoch+1:02d}/{self.n_epochs}  hinge+L2 loss: {loss:.3f}\")\n",
        "\n",
        "            if self.early_stopping:\n",
        "                if loss < best - 1e-4:\n",
        "                    best = loss; bad = 0\n",
        "                else:\n",
        "                    bad += 1\n",
        "                    if bad >= self.es_patience:\n",
        "                        print(\"Early stopping.\")\n",
        "                        break\n",
        "\n",
        "        return self\n",
        "\n",
        "    def decision_function(self, X):\n",
        "        # ------------------------------------------------\n",
        "        # TODO Refer to the linear decision function equation in the cell above (Exercise 3)\n",
        "        # ------------------------------------------------\n",
        "        return X @  +\n",
        "        # ----------------------------\n",
        "        # Implementation Ends Here\n",
        "        # ----------------------------\n",
        "\n",
        "    def predict(self, X):\n",
        "        # ------------------------------------------------\n",
        "        # TODO We want to utilize the decision_function we defined in LinearSVMScratch. (Exercise 4)\n",
        "        # ------------------------------------------------\n",
        "        s = self.\n",
        "        # ----------------------------\n",
        "        # Implementation Ends Here\n",
        "        # ----------------------------\n",
        "        return (s >= 0.0).astype(int)\n",
        "\n",
        "# ---------------- Pipeline: TF-IDF -> SVD(200) -> Scaler ----------------\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=60000, ngram_range=(1,2), min_df=2)\n",
        "svd = TruncatedSVD(n_components=200, random_state=7)\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "\n",
        "# Build numeric features\n",
        "Xtr_sparse = vectorizer.fit_transform(X_train_text)\n",
        "Xte_sparse = vectorizer.transform(X_test_text)\n",
        "\n",
        "Xtr_red = svd.fit_transform(Xtr_sparse)\n",
        "Xte_red = svd.transform(Xte_sparse)\n",
        "\n",
        "Xtr = scaler.fit_transform(Xtr_red)\n",
        "Xte = scaler.transform(Xte_red)\n",
        "\n",
        "# ---------------- Train & Evaluate ----------------\n",
        "for lr_val in [0.1,0.05,0.01,0.001]:\n",
        "  svm = LinearSVMScratch(C=1.0, lr=lr_val, n_epochs=10, lr_decay=0.02, early_stopping=True, es_patience=2, random_state=7)\n",
        "  svm.fit(Xtr, y_train)\n",
        "\n",
        "  y_pred = svm.predict(Xtr)\n",
        "  print(\"From-scratch Linear SVM â€” IMDB (SVD-200)\")\n",
        "  print(\"Train accuracy:\", accuracy_score(y_train, y_pred))\n",
        "  print(classification_report(y_train, y_pred, target_names=['neg','pos'], digits=3))\n",
        "\n",
        "  # Plot training loss\n",
        "  plt.figure()\n",
        "  plt.plot(svm.loss_curve_)\n",
        "  plt.title(\"Hinge Loss (with L2) per Epoch â€” From-scratch Linear SVM\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svm = LinearSVMScratch(C=1.0, lr=lr_val, n_epochs=10, lr_decay=0.02, early_stopping=True, es_patience=2, random_state=7)\n",
        "svm.fit(Xtr, y_train)\n",
        "\n",
        "y_pred = svm.predict(Xte)\n",
        "print(\"From-scratch Linear SVM â€” IMDB (SVD-200)\")\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=['neg','pos'], digits=3))\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure()\n",
        "plt.plot(svm.loss_curve_)\n",
        "plt.title(\"Hinge Loss (with L2) per Epoch â€” From-scratch Linear SVM\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WBE5RMfOdIv5"
      },
      "id": "WBE5RMfOdIv5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# TODO We will be utilizing the LinearSVC from sklearn, we will use a C=1, max_iter=2000, dual=True and random_state=7 (Exercise 5)\n",
        "# First we will train on our data Xtr\n",
        "# After we want to do predictions on the test data\n",
        "# ------------------------------------------------\n",
        "lin =\n",
        "lin.\n",
        "pred_lin = lin.\n",
        "# ----------------------------\n",
        "# Implementation Ends Here\n",
        "# ----------------------------\n",
        "\n",
        "print(\"LinearSVC â€” test accuracy:\", accuracy_score(y_test, pred_lin))"
      ],
      "metadata": {
        "id": "WmqcneLdtTA8"
      },
      "id": "WmqcneLdtTA8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§  Mathematical Foundations of the Polynomial Kernel SVM\n",
        "\n",
        "### 1. Problem Setup\n",
        "\n",
        "We are given training data  \n",
        "$\n",
        "\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n, \\quad \\mathbf{x}_i \\in \\mathbb{R}^d, \\; y_i \\in \\{-1, +1\\}.\n",
        "$\n",
        "\n",
        "The goal of a Support Vector Machine (SVM) is to find a decision function  \n",
        "**Decision Function**: $\n",
        "f(\\mathbf{x}) = \\sum_{j=1}^n \\beta_j K(\\mathbf{x}_j, \\mathbf{x}) + b\n",
        "$\n",
        "\n",
        "that separates the two classes with the largest possible margin.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. The Polynomial Kernel\n",
        "\n",
        "Instead of using raw dot products, we map data into a higher-dimensional space via the **polynomial kernel**:\n",
        "$\n",
        "K(\\mathbf{x}, \\mathbf{z}) = (G\\, \\mathbf{x}^\\top \\mathbf{z} + c_0)^{d},\n",
        "$\n",
        "where:\n",
        "\n",
        "- $( G > 0 )$ controls how strongly similarity depends on the dot product  \n",
        "- $( c_0 )$ (often called `coef0` in code) shifts the kernel to include lower-order terms  \n",
        "- $( d )$ is the **degree** of the polynomial and controls nonlinearity\n",
        "\n",
        "This kernel implicitly represents all polynomial interactions between features up to degree $(d)$.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Regularized Risk (Squared Hinge Form)\n",
        "\n",
        "We minimize the **regularized empirical risk** in the Reproducing Kernel Hilbert Space (RKHS):\n",
        "\n",
        "$\n",
        "J(\\boldsymbol{\\beta}, b) = \\frac{\\lambda}{2}\\,\\boldsymbol{\\beta}^\\top K \\boldsymbol{\\beta} + \\frac{1}{n}\\sum_{i=1}^n \\max(0,\\, 1 - y_i f_i)^2, \\text{where} \\quad f_i = (K\\boldsymbol{\\beta})_i + b \\quad \\text{and} \\quad K_{ij} = K(\\mathbf{x}_i, \\mathbf{x}_j). $\n",
        "\n",
        "- The first term $( \\frac{\\lambda}{2}\\boldsymbol{\\beta}^\\top K \\boldsymbol{\\beta} )$ is a **regularizer** that penalizes model complexity.  \n",
        "- The second term is the **squared hinge loss**, which penalizes samples that violate the margin constraint $( y_i f_i \\ge 1 )$.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Gradient Derivations\n",
        "\n",
        "Let $( r_i = \\max(0, 1 - y_i f_i) )$.  \n",
        "Only samples with $( y_i f_i < 1 )$ contribute to the gradient.\n",
        "\n",
        "**Gradients:**\n",
        "\n",
        "$ \\nabla_{\\boldsymbol{\\beta}} J = \\lambda K \\boldsymbol{\\beta} - \\frac{2}{n}\\, K(\\mathbf{y} \\odot \\mathbf{r}), \\nabla_b J = -\\frac{2}{n}\\sum_{i=1}^n y_i r_i.\n",
        "$\n",
        "\n",
        "Here $( \\odot )$ denotes element-wise multiplication.\n",
        "\n",
        "These are the update directions used by the optimizer in the implementation.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Optimization via Gradient Descent\n",
        "\n",
        "We update parameters using gradient-based optimization (Adam in the code):\n",
        "\n",
        "$\n",
        "\\boldsymbol{\\beta} \\leftarrow \\boldsymbol{\\beta} - \\eta\\, \\nabla_{\\boldsymbol{\\beta}} J,\n",
        "\\quad\n",
        "b \\leftarrow b - \\eta\\, \\nabla_b J.\n",
        "$\n",
        "\n",
        "Adam adaptively rescales these gradients using running estimates of their first and second moments.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Decision Function After Training\n",
        "\n",
        "The learned decision function becomes:\n",
        "\n",
        "$\n",
        "f(\\mathbf{x}) = \\sum_{j=1}^n \\beta_j K(\\mathbf{x}_j, \\mathbf{x}) + b,\n",
        "$\n",
        "and the prediction rule is:\n",
        "$\n",
        "\\hat{y} = \\operatorname{sign}(f(\\mathbf{x})).\n",
        "$\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Interpretation\n",
        "\n",
        "| Symbol | Meaning | Notes |\n",
        "|:-------|:---------|:------|\n",
        "| $(K(\\mathbf{x}, \\mathbf{z})$) | Polynomial kernel | Expands dot products to polynomial feature space |\n",
        "| $(\\boldsymbol{\\beta}$) | Coefficients of the kernel expansion | Similar to dual variables \\( \\alpha \\) in classic SVM |\n",
        "| $(b$) | Bias/intercept | Shifts the decision boundary |\n",
        "| $(\\lambda$) | Regularization strength | Controls smoothness of boundary |\n",
        "| $(r_i$) | Margin violation residual | Non-zero if $( y_i f_i < 1 $) |\n",
        "\n",
        "The polynomial kernel allows the SVM to form **curved, nonlinear decision boundaries**.  \n",
        "Higher degrees \\(d\\) increase flexibility, but also the risk of overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. Summary of the Objective\n",
        "\n",
        "The optimization performed in code minimizes:\n",
        "$ \\boxed{ \\min_{\\boldsymbol{\\beta}, b} \\left[ \\frac{\\lambda}{2}\\boldsymbol{\\beta}^\\top K\\boldsymbol{\\beta} + \\frac{1}{n}\\sum_{i=1}^n \\max(0, 1 - y_i((K\\boldsymbol{\\beta})_i + b))^2 \\right] }\n",
        "$\n",
        "\n",
        "This is a smooth, differentiable version of the kernel SVM objective that can be solved by gradient descent without SMO or quadratic programming.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PqtSuCst2y1w"
      },
      "id": "PqtSuCst2y1w"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons, make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ------------------- Your gradient-based poly SVM (no SMO) ------------------- #\n",
        "class PolyKernelSVM_GD:\n",
        "    \"\"\"\n",
        "    Kernel SVM via gradient descent (squared hinge in a kernelized primal).\n",
        "    J(Î², b) = (Î»/2) * Î²^T K Î² + (1/n) * Î£_i max(0, 1 - y_i * (KÎ² + b)_i)^2\n",
        "    Decision: f(x) = Î£_j Î²_j K(x_j, x) + b\n",
        "    Labels in {-1, +1}.\n",
        "    \"\"\"\n",
        "    def __init__(self, degree=3, gamma=None, coef0=1.0, lam=1e-2,\n",
        "                 lr=5e-2, epochs=2000, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-8,\n",
        "                 random_state=0, verbose=False):\n",
        "        self.degree = int(degree)\n",
        "        self.gamma = (1.0) if gamma is None else float(gamma)\n",
        "        self.coef0 = float(coef0)\n",
        "        self.lam = float(lam)\n",
        "        self.lr = float(lr)\n",
        "        self.epochs = int(epochs)\n",
        "        self.beta1 = float(adam_beta1)\n",
        "        self.beta2 = float(adam_beta2)\n",
        "        self.adam_eps = float(adam_eps)\n",
        "        self.rs = np.random.RandomState(random_state)\n",
        "        self.verbose = verbose\n",
        "        self.X = None; self.beta = None; self.b = 0.0; self.K = None\n",
        "\n",
        "    def _poly_kernel(self, X, Z):\n",
        "        X = np.asarray(X, dtype=float); Z = np.asarray(Z, dtype=float)\n",
        "        g = self.gamma if self.gamma is not None else 1.0 / X.shape[1]\n",
        "        # ------------------------------------------------\n",
        "        # TODO Refer to the Polynomial Kernel section in the cell above (Exercise 6)\n",
        "        # ------------------------------------------------\n",
        "        return ( * ( @ Z.T) + self.coef0) ** self.\n",
        "        # ----------------------------\n",
        "        # Implementation Ends Here\n",
        "        # ----------------------------\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        y = (np.asarray(y, dtype=float))\n",
        "        assert set(np.unique(y)).issubset({-1.0, 1.0}), \"Labels must be {-1,+1}\"\n",
        "        n = X.shape[0]\n",
        "        self.X = X\n",
        "        # ------------------------------------------------\n",
        "        # TODO Refer to the Polynomial Kernel section in the cell above (Exercise 7)\n",
        "        # Hint we might use X twice (since we are building the training kernel matrix between every pair of training samples)\n",
        "        # ------------------------------------------------\n",
        "        self.K = self._poly_kernel(X, )\n",
        "        # ----------------------------\n",
        "        # Implementation Ends Here\n",
        "        # ----------------------------\n",
        "        self.beta = np.zeros(n); self.b = 0.0\n",
        "\n",
        "        m_beta = np.zeros_like(self.beta); v_beta = np.zeros_like(self.beta)\n",
        "        m_b = 0.0; v_b = 0.0\n",
        "\n",
        "        for t in range(1, self.epochs + 1):\n",
        "            # ------------------------------------------------\n",
        "            #TODO Refer to the decision function defined in the 1) Problem Setup in the cell above (Exercise 8)\n",
        "            # ------------------------------------------------\n",
        "            f = self.K @ self. + self.b\n",
        "            # ----------------------------\n",
        "            # Implementation Ends Here\n",
        "            # ----------------------------\n",
        "            margin = y * f\n",
        "            r = np.clip(1.0 - margin, 0.0, None)\n",
        "\n",
        "            grad_beta = self.lam * (self.K @ self.beta) - (2.0 / n) * (self.K @ (y * r))\n",
        "            grad_b = -(2.0 / n) * np.sum(y * r)\n",
        "\n",
        "            m_beta = self.beta1*m_beta + (1-self.beta1)*grad_beta\n",
        "            v_beta = self.beta2*v_beta + (1-self.beta2)*(grad_beta**2)\n",
        "            m_b = self.beta1*m_b + (1-self.beta1)*grad_b\n",
        "            v_b = self.beta2*v_b + (1-self.beta2)*(grad_b**2)\n",
        "\n",
        "            m_beta_hat = m_beta / (1 - self.beta1**t)\n",
        "            v_beta_hat = v_beta / (1 - self.beta2**t)\n",
        "            m_b_hat = m_b / (1 - self.beta1**t)\n",
        "            v_b_hat = v_b / (1 - self.beta2**t)\n",
        "\n",
        "            self.beta -= self.lr * m_beta_hat / (np.sqrt(v_beta_hat) + self.adam_eps)\n",
        "            self.b    -= self.lr * m_b_hat / (np.sqrt(v_b_hat) + self.adam_eps)\n",
        "\n",
        "            if self.verbose and (t % max(1, self.epochs // 10) == 0 or t == 1):\n",
        "                reg = 0.5 * self.lam * (self.beta @ (self.K @ self.beta))\n",
        "                loss = (r ** 2).mean()\n",
        "                print(f\"[{t:5d}/{self.epochs}] loss={loss:.4f} reg={reg:.4f} obj={loss+reg:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def decision_function(self, Xq):\n",
        "        Kq = self._poly_kernel(np.asarray(Xq, dtype=float), self.X)\n",
        "        # ------------------------------------------------\n",
        "        #TODO Refer to the decision function after training section defined in the cell above (Exercise 9)\n",
        "        # ------------------------------------------------\n",
        "        return  @ self.beta + self.b\n",
        "        # ----------------------------\n",
        "        # Implementation Ends Here\n",
        "        # ----------------------------\n",
        "\n",
        "    def predict(self, Xq):\n",
        "        return np.where(self.decision_function(Xq) >= 0.0, 1.0, -1.0)\n",
        "\n",
        "# ------------------- Helpers ------------------- #\n",
        "def plot_side_by_side(models, titles, X, y):\n",
        "    x_min, x_max = X[:,0].min()-0.8, X[:,0].max()+0.8\n",
        "    y_min, y_max = X[:,1].min()-0.8, X[:,1].max()+0.8\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 400),\n",
        "                         np.linspace(y_min, y_max, 400))\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    fig, axes = plt.subplots(1, len(models), figsize=(6*len(models), 5))\n",
        "    if len(models) == 1: axes = [axes]\n",
        "    for ax, model, title in zip(axes, models, titles):\n",
        "        zz = model.decision_function(grid).reshape(xx.shape)\n",
        "        ax.contourf(xx, yy, np.sign(zz), levels=[-np.inf,0,np.inf], alpha=0.2)\n",
        "        cs = ax.contour(xx, yy, zz, levels=[-1,0,1], linestyles=['--','-','--'])\n",
        "        ax.clabel(cs, inline=True, fontsize=8)\n",
        "        ax.scatter(X[:,0], X[:,1], c=(y>0).astype(int), cmap='bwr', edgecolor='k', s=30)\n",
        "        ax.set_title(title); ax.set_xlabel(\"x1\"); ax.set_ylabel(\"x2\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# ------------------- Config ------------------- #\n",
        "DATASET = \"moons\"      # \"moons\" or \"circles\"\n",
        "NOISE = 0.25\n",
        "N_SAMPLES = 600\n",
        "TEST_SIZE = 0.3\n",
        "RS = 42\n",
        "\n",
        "DEGREE = 3\n",
        "GAMMA = 1.0\n",
        "COEF0 = 1.0\n",
        "\n",
        "# Î» â†” C rough mapping:\n",
        "# For squared hinge with averaging over n, a common heuristic is C â‰ˆ 1/(Î» n).\n",
        "# You can tweak around this to match margins more closely.\n",
        "def lambda_to_C(lam, n): return 1.0 / (lam * n)\n",
        "\n",
        "LAM = 1e-2\n",
        "EPOCHS = 2000\n",
        "LR = 5e-2\n",
        "\n",
        "# ------------------- Data ------------------- #\n",
        "if DATASET == \"moons\":\n",
        "    X, y01 = make_moons(n_samples=N_SAMPLES, noise=NOISE, random_state=RS)\n",
        "elif DATASET == \"circles\":\n",
        "    X, y01 = make_circles(n_samples=N_SAMPLES, noise=NOISE, factor=0.5, random_state=RS)\n",
        "else:\n",
        "    raise ValueError(\"DATASET must be 'moons' or 'circles'.\")\n",
        "\n",
        "y = 2*y01 - 1  # {0,1} -> {-1,+1}\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=TEST_SIZE, random_state=RS, stratify=y01)\n",
        "\n",
        "# ------------------- Train: custom ------------------- #\n",
        "t0 = time()\n",
        "custom = PolyKernelSVM_GD(\n",
        "    degree=DEGREE, gamma=GAMMA, coef0=COEF0,\n",
        "    lam=LAM, lr=LR, epochs=EPOCHS, verbose=False, random_state=RS\n",
        ").fit(X_tr, y_tr)\n",
        "t_custom = time() - t0\n",
        "\n",
        "# ------------------- Train: sklearn ------------------- #\n",
        "C = lambda_to_C(LAM, n=X_tr.shape[0])\n",
        "t1 = time()\n",
        "\n",
        "# ------------------------------------------------\n",
        "# TODO We will be utilizing the SVC from sklearn (sklearn.svm.SVC), we will use a kernel=\"poly\", degree=DEGREE, gamma=GAMMA, coef0=COEF0, C=C (Exercise 10)\n",
        "# We will train on the X_tr data and y_tr, remember sklearn expects y_tr to be {0,1}\n",
        "# ------------------------------------------------\n",
        "sk =\n",
        "sk.fit(, )  # sklearn expects {0,1}\n",
        "# ----------------------------\n",
        "# Implementation Ends Here\n",
        "# ----------------------------\n",
        "t_sk = time() - t1\n",
        "\n",
        "# ------------------- Evaluate ------------------- #\n",
        "def acc(model, X, y):\n",
        "    yp = model.predict(X)\n",
        "    # unify label spaces\n",
        "    if yp.min() >= 0:  # sklearn: {0,1}\n",
        "        yp = np.where(yp==1, 1, -1)\n",
        "    return (yp == y).mean()\n",
        "\n",
        "train_acc_custom = acc(custom, X_tr, y_tr)\n",
        "test_acc_custom  = acc(custom, X_te, y_te)\n",
        "\n",
        "train_acc_sk = acc(sk, X_tr, y_tr)\n",
        "test_acc_sk  = acc(sk, X_te, y_te)\n",
        "\n",
        "print(f\"Dataset={DATASET}, degree={DEGREE}, gamma={GAMMA}, coef0={COEF0}\")\n",
        "print(f\"Custom-GD:  lam={LAM}  epochs={EPOCHS}  lr={LR}  time={t_custom:.3f}s  \"\n",
        "      f\"train_acc={train_acc_custom:.3f}  test_acc={test_acc_custom:.3f}\")\n",
        "print(f\"sklearn SVC: Câ‰ˆ{C:.4f} time={t_sk:.3f}s  \"\n",
        "      f\"train_acc={train_acc_sk:.3f}  test_acc={test_acc_sk:.3f}\")\n",
        "\n",
        "# ------------------- Plot side-by-side ------------------- #\n",
        "# Wrap sklearn model with a decision_function in {-1,+1} convention\n",
        "class SKWrap:\n",
        "    def __init__(self, svc): self.svc = svc\n",
        "    def decision_function(self, Xq):\n",
        "        return self.svc.decision_function(Xq)\n",
        "    def predict(self, Xq):\n",
        "        return np.where(self.svc.decision_function(Xq) >= 0, 1, -1)\n",
        "\n",
        "plot_side_by_side(\n",
        "    [custom, SKWrap(sk)],\n",
        "    [f\"Custom Poly SVM (Î»={LAM})\", f\"sklearn SVC (Câ‰ˆ{C:.2g})\"],\n",
        "    X_tr, y_tr\n",
        ")\n"
      ],
      "metadata": {
        "id": "uLr18w6PULey"
      },
      "id": "uLr18w6PULey",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Questions and Answers** (10 points)\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Regularization Parameter**  \n",
        "\n",
        "What role does the regularization parameter (C) play in a Linear SVM?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "### **2. Linear SVM's**\n",
        "\n",
        "Why are Linear SVMs typically preferred for high-dimensional, sparse datasets (like TF-IDF text features)?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "### **3. Polynomial SVM's**\n",
        "How do the parameters degree (d), gamma (G), and coef0 (câ‚€) influence the decision boundary?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "### **4. Overfitting Polynomial SVM**\n",
        "Why might a Polynomial SVM overfit more easily than a Linear SVM?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "### **5. Polynomial Kernel Purpose**\n",
        "Why does a Polynomial kernel make linear classifiers capable of separating nonlinear data?\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "87m-wA1qlsAI"
      },
      "id": "87m-wA1qlsAI"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}